"""
Ba7ath LLM Analysis Service
============================
Service d'analyse croisÃ©e des donnÃ©es Ahlya/JORT/RNE via Google Gemini 1.5 Flash.

Ce module gÃ¨re :
1. La connexion sÃ©curisÃ©e Ã  l'API Google Generative AI.
2. La construction de prompts structurÃ©s pour l'analyse OSINT.
3. La validation dÃ©terministe des rÃ©ponses au format JSON.
4. Une gestion d'erreurs granulaire pour la traÃ§abilitÃ© journalistique.
"""

import os
import json
import logging
from datetime import datetime

import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

# Configuration du logging spÃ©cifique au module Ba7ath
logger = logging.getLogger("ba7ath.llm")
logger.setLevel(logging.INFO)

# â”€â”€ System Prompt (Expert Investigation) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ¯Ù‚ÙŠÙ‚ Ù…Ø­Ù‚Ù‚ ÙÙŠ Ù…Ø´Ø±ÙˆØ¹ 'Ø¨Ø­Ø«' (Ba7ath). Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯Ù‚Ø© Ù…ØªÙ†Ø§Ù‡ÙŠØ©.

Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:
- "Ø´Ø±ÙƒØ© Ø£Ù‡Ù„ÙŠØ©" (Entreprise Citoyenne) Ù‡ÙŠ ÙƒÙŠØ§Ù† Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø£ÙÙ†Ø´Ø¦ Ø¨Ù…ÙˆØ¬Ø¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø¹Ø¯Ø¯ 20 Ù„Ø³Ù†Ø© 2022.
- "Ø§Ù„Ø±Ø§Ø¦Ø¯ Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ© Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©" (JORT) Ù‡Ùˆ Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ø±Ø³Ù…ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ÙÙŠÙ‡ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù† ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø´Ø±ÙƒØ§Øª.
- "Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ù…Ø¤Ø³Ø³Ø§Øª" (RNE) Ù‡Ùˆ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ© Ø§Ù„Ø±Ø³Ù…ÙŠØ©.
- "Ø§Ù„Ù…Ø¹Ø±Ù‘Ù Ø§Ù„Ø¬Ø¨Ø§Ø¦ÙŠ" (Matricule Fiscal) Ù‡Ùˆ Ø±Ù‚Ù… Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ.
- "Ø§Ù„ÙˆÙ„Ø§ÙŠØ©" (Gouvernorat) Ù‡ÙŠ Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ© ÙÙŠ ØªÙˆÙ†Ø³ (24 ÙˆÙ„Ø§ÙŠØ©).

Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:
1. Ù„Ø§ ØªØ³ØªÙ†ØªØ¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.
2. Ø¥Ø°Ø§ ÙˆØ¬Ø¯ Ø§Ø®ØªÙ„Ø§Ù Ø¨ÙŠÙ† Ø§Ù„Ù…ØµØ§Ø¯Ø±ØŒ ØµÙ†ÙÙ‡ ÙƒÙ€ 'ØªØ¶Ø§Ø±Ø¨' (Conflict).
3. Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù‡ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø±ØµÙŠÙ†Ø© (MSA).
4. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ (summary_ar) Ù…Ù‡Ù†ÙŠÙ‹Ø§ØŒ Ù…Ø¨Ø§Ø´Ø±Ù‹Ø§ØŒ ÙˆÙ…Ø¨Ù†ÙŠÙ‹Ø§ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.
5. Ù„Ø§ ØªØ¶Ù Ù†ØµÙˆØµÙ‹Ø§ ØªÙØ³ÙŠØ±ÙŠØ© Ø®Ø§Ø±Ø¬ Ù‡ÙŠÙƒÙ„ JSON Ø§Ù„Ù…Ø·Ù„ÙˆØ¨."""

# â”€â”€ JSON Schema (embedded in prompt, NOT in GenerationConfig) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NOTE: response_schema forces v1beta routing which causes 404 on Render.
# We embed the schema in the prompt instead and only use response_mime_type.

# â”€â”€ Fallback response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _fallback_response(error_type: str, detail: str = "") -> dict:
    """GÃ©nÃ¨re une rÃ©ponse JSON de secours en cas d'indisponibilitÃ© du LLM."""
    return {
        "match_score": 0,
        "status": "Pending",
        "findings": [],
        "red_flags": [],
        "summary_ar": f"ØªØ¹Ø°Ù‘Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {error_type}. {detail}".strip(),
        "_error": error_type,
        "_detail": detail,
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â–ˆâ–ˆ  LLM ANALYSIS SERVICE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LLMAnalysisService:
    """
    Service d'analyse utilisant Google Gemini 1.5 Flash pour le cross-referencing.
    ConfigurÃ© pour le dÃ©terminisme total (Temp=0).
    """

    def __init__(self):
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            logger.warning("GEMINI_API_KEY not set â€” LLM analysis will be unavailable")
            self.model = None
            return

        try:
            # Initialisation de l'API avec la clÃ© d'environnement
            genai.configure(api_key=api_key)

            # Utilisation du nom de modÃ¨le racine pour Ã©viter les erreurs de version v1beta sur Render
            model_id = "gemini-1.5-flash"

            self.model = genai.GenerativeModel(
                model_name=model_id,
                system_instruction=SYSTEM_PROMPT
            )
            
            # Configuration de gÃ©nÃ©ration â€” JSON via v1 stable (pas v1beta)
            # IMPORTANT: response_schema est volontairement ABSENT car il
            # force le routage vers v1beta, qui renvoie 404 sur Render.
            # Le schÃ©ma JSON est injectÃ© directement dans le prompt.
            self.generation_config = genai.GenerationConfig(
                temperature=0.0,
                top_p=1,
                top_k=1,
                response_mime_type="application/json",
            )
            
            logger.info(f"âœ… LLMAnalysisService initialized â€” model: {model_id}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Gemini Model: {str(e)}")
            self.model = None

    @staticmethod
    def _build_prompt(ahlya_data: dict, jort_data: dict, rne_data: dict) -> str:
        """Construit un prompt structurÃ© avec les trois sources de donnÃ©es."""
        
        def format_section(data): 
            return json.dumps(data, ensure_ascii=False, indent=2) if data else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª"

        return f"""Ù‚Ù… Ø¨Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙƒØ© Ø§Ù„Ø£Ù‡Ù„ÙŠØ© Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ÙˆÙ„: Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ù‡Ù„ÙŠØ© (Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØµØ±ÙŠØ­ÙŠØ©)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{format_section(ahlya_data)}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„Ø±Ø§Ø¦Ø¯ Ø§Ù„Ø±Ø³Ù…ÙŠ (JORT)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{format_section(jort_data)}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø«Ø§Ù„Ø«: Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ù…Ø¤Ø³Ø³Ø§Øª (RNE)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{format_section(rne_data)}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Ù‚Ø§Ø±Ù† Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠØŒ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ØŒ ÙˆØ§Ù„ÙˆÙ„Ø§ÙŠØ©.
2. ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® ÙˆØ§Ù„Ù…Ø¹Ø±Ù‘Ù Ø§Ù„Ø¬Ø¨Ø§Ø¦ÙŠ.
3. Ø­Ø¯Ø¯ Ø£ÙŠ ØªØ¶Ø§Ø±Ø¨Ø§Øª (Conflicts) Ø£Ùˆ Ù†Ù‚Ø§Ø· Ù…Ø´Ø¨ÙˆÙ‡Ø©.
4. Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· ÙˆÙÙ‚ Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ù„Ø¶Ø¨Ø·:

{{
  "match_score": <Ø¹Ø¯Ø¯ ØµØ­ÙŠØ­ Ù…Ù† 0 Ø¥Ù„Ù‰ 100>,
  "status": "Verified" Ø£Ùˆ "Suspicious" Ø£Ùˆ "Conflict",
  "findings": ["Ù†Ù‚Ø·Ø© ØªØ·Ø§Ø¨Ù‚ 1", "Ù†Ù‚Ø·Ø© ØªØ·Ø§Ø¨Ù‚ 2"],
  "red_flags": ["ØªØ¬Ø§ÙˆØ² 1", "ØªØ¬Ø§ÙˆØ² 2"],
  "summary_ar": "Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ù‡Ù†Ø§"
}}"""

    async def analyze_cross_check(self, ahlya_data: dict, jort_data: dict, rne_data: dict) -> dict:
        """ExÃ©cute l'analyse croisÃ©e via Gemini avec gestion d'erreurs granulaire."""
        
        company_name = ahlya_data.get("name", "Unknown")

        if not self.model:
            logger.error(f"LLM analysis skipped for '{company_name}': no API key")
            return _fallback_response("no_api_key", "GEMINI_API_KEY ØºÙŠØ± Ù…ÙØ¹ÙÙŠÙÙ‘Ù†")

        logger.info(f"ğŸ” Starting LLM cross-check for: {company_name}")
        start_time = datetime.now()
        prompt = self._build_prompt(ahlya_data, jort_data, rne_data)

        try:
            # Appel Ã  l'API Gemini
            response = self.model.generate_content(
                prompt,
                generation_config=self.generation_config
            )
            
            # Parsing de la rÃ©ponse JSON
            result = json.loads(response.text)

            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… Analysis complete for '{company_name}' in {elapsed:.1f}s")
            return result

        except google_exceptions.ResourceExhausted as e:
            logger.warning(f"âš ï¸ Rate-limit Gemini (429) for '{company_name}': {e}")
            return _fallback_response("rate_limited", "Ø§Ù„Ø®Ø¯Ù…Ø© Ù…Ø´ØºÙˆÙ„Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§.")

        except google_exceptions.InvalidArgument as e:
            logger.error(f"âŒ InvalidArgument for '{company_name}': {e}")
            return _fallback_response("invalid_argument", str(e))

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONDecodeError for '{company_name}': {e}")
            return _fallback_response("json_parse_error", "ØªØ¹Ø°Ù‘Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")

        except Exception as e:
            logger.error(f"âŒ Unexpected error for '{company_name}': {e}")
            return _fallback_response("unexpected_error", str(e))

# Instance unique du service
llm_service = LLMAnalysisService()