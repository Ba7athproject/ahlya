"""
Ba7ath LLM Analysis Service
============================
Service d'analyse croisÃ©e des donnÃ©es Ahlya/JORT/RNE via Google Gemini.

Ce module utilise l'API REST Gemini DIRECTEMENT via httpx (pas le SDK
google-generativeai) pour forcer l'utilisation de l'endpoint v1 stable
et Ã©viter le routage automatique vers v1beta qui provoque des erreurs
404 sur Render et autres plateformes cloud.
"""

import os
import json
import logging
from datetime import datetime

import httpx

# Configuration du logging spÃ©cifique au module Ba7ath
logger = logging.getLogger("ba7ath.llm")
logger.setLevel(logging.INFO)

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

GEMINI_API_BASE = "https://generativelanguage.googleapis.com/v1beta"
GEMINI_MODEL = "gemini-2.0-flash"
GEMINI_ENDPOINT = f"{GEMINI_API_BASE}/models/{GEMINI_MODEL}:generateContent"

# â”€â”€ System Prompt (Expert Investigation) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ¯Ù‚ÙŠÙ‚ Ù…Ø­Ù‚Ù‚ ÙÙŠ Ù…Ø´Ø±ÙˆØ¹ 'Ø¨Ø­Ø«' (Ba7ath). Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯Ù‚Ø© Ù…ØªÙ†Ø§Ù‡ÙŠØ©.

Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:
- "Ø´Ø±ÙƒØ© Ø£Ù‡Ù„ÙŠØ©" (Entreprise Citoyenne) Ù‡ÙŠ ÙƒÙŠØ§Ù† Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø£ÙÙ†Ø´Ø¦ Ø¨Ù…ÙˆØ¬Ø¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø¹Ø¯Ø¯ 20 Ù„Ø³Ù†Ø© 2022.
- "Ø§Ù„Ø±Ø§Ø¦Ø¯ Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ© Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©" (JORT) Ù‡Ùˆ Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ø±Ø³Ù…ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ÙÙŠÙ‡ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù† ØªØ£Ø³ÙŠØ³ Ø§Ù„Ø´Ø±ÙƒØ§Øª.
- "Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ù…Ø¤Ø³Ø³Ø§Øª" (RNE) Ù‡Ùˆ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ© Ø§Ù„Ø±Ø³Ù…ÙŠØ©.
- "Ø§Ù„Ù…Ø¹Ø±Ù‘Ù Ø§Ù„Ø¬Ø¨Ø§Ø¦ÙŠ" (Matricule Fiscal) Ù‡Ùˆ Ø±Ù‚Ù… Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ.
- "Ø§Ù„ÙˆÙ„Ø§ÙŠØ©" (Gouvernorat) Ù‡ÙŠ Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ© ÙÙŠ ØªÙˆÙ†Ø³ (24 ÙˆÙ„Ø§ÙŠØ©).

Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:
1. Ù„Ø§ ØªØ³ØªÙ†ØªØ¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.
2. Ø¥Ø°Ø§ ÙˆØ¬Ø¯ Ø§Ø®ØªÙ„Ø§Ù Ø¨ÙŠÙ† Ø§Ù„Ù…ØµØ§Ø¯Ø±ØŒ ØµÙ†ÙÙ‡ ÙƒÙ€ 'ØªØ¶Ø§Ø±Ø¨' (Conflict).
3. Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù‡ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø±ØµÙŠÙ†Ø© (MSA).
4. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ (summary_ar) Ù…Ù‡Ù†ÙŠÙ‹Ø§ØŒ Ù…Ø¨Ø§Ø´Ø±Ù‹Ø§ØŒ ÙˆÙ…Ø¨Ù†ÙŠÙ‹Ø§ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.
5. Ù„Ø§ ØªØ¶Ù Ù†ØµÙˆØµÙ‹Ø§ ØªÙØ³ÙŠØ±ÙŠØ© Ø®Ø§Ø±Ø¬ Ù‡ÙŠÙƒÙ„ JSON Ø§Ù„Ù…Ø·Ù„ÙˆØ¨."""

# â”€â”€ Fallback response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _fallback_response(error_type: str, detail: str = "") -> dict:
    """GÃ©nÃ¨re une rÃ©ponse JSON de secours en cas d'indisponibilitÃ© du LLM."""
    return {
        "match_score": 0,
        "status": "Pending",
        "findings": [],
        "red_flags": [],
        "summary_ar": f"ØªØ¹Ø°Ù‘Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {error_type}. {detail}".strip(),
        "_error": error_type,
        "_detail": detail,
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â–ˆâ–ˆ  LLM ANALYSIS SERVICE (Direct REST API â€” no SDK)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LLMAnalysisService:
    """
    Service d'analyse utilisant l'API REST Gemini directement.
    Contourne le SDK google-generativeai pour Ã©viter le routage v1beta.
    ConfigurÃ© pour le dÃ©terminisme total (Temp=0).
    """

    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            logger.warning("âš ï¸ GEMINI_API_KEY not set â€” LLM analysis will be unavailable")
        else:
            logger.info(f"âœ… LLMAnalysisService initialized â€” model: {GEMINI_MODEL} (REST API direct)")

    @staticmethod
    def _build_prompt(ahlya_data: dict, jort_data: dict, rne_data: dict) -> str:
        """Construit un prompt structurÃ© avec les trois sources de donnÃ©es."""

        def fmt(data):
            return json.dumps(data, ensure_ascii=False, indent=2) if data else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª"

        return f"""Ù‚Ù… Ø¨Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙƒØ© Ø§Ù„Ø£Ù‡Ù„ÙŠØ© Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ÙˆÙ„: Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ù‡Ù„ÙŠØ© (Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØµØ±ÙŠØ­ÙŠØ©)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{fmt(ahlya_data)}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„Ø±Ø§Ø¦Ø¯ Ø§Ù„Ø±Ø³Ù…ÙŠ (JORT)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{fmt(jort_data)}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø«Ø§Ù„Ø«: Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ù…Ø¤Ø³Ø³Ø§Øª (RNE)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{fmt(rne_data)}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Ù‚Ø§Ø±Ù† Ø§Ù„Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø§Ø±ÙŠØŒ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ØŒ ÙˆØ§Ù„ÙˆÙ„Ø§ÙŠØ©.
2. ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® ÙˆØ§Ù„Ù…Ø¹Ø±Ù‘Ù Ø§Ù„Ø¬Ø¨Ø§Ø¦ÙŠ.
3. Ø­Ø¯Ø¯ Ø£ÙŠ ØªØ¶Ø§Ø±Ø¨Ø§Øª (Conflicts) Ø£Ùˆ Ù†Ù‚Ø§Ø· Ù…Ø´Ø¨ÙˆÙ‡Ø©.
4. Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· ÙˆÙÙ‚ Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ù„Ø¶Ø¨Ø·:

{{
  "match_score": <Ø¹Ø¯Ø¯ ØµØ­ÙŠØ­ Ù…Ù† 0 Ø¥Ù„Ù‰ 100>,
  "status": "Verified" Ø£Ùˆ "Suspicious" Ø£Ùˆ "Conflict",
  "findings": ["Ù†Ù‚Ø·Ø© ØªØ·Ø§Ø¨Ù‚ 1", "Ù†Ù‚Ø·Ø© ØªØ·Ø§Ø¨Ù‚ 2"],
  "red_flags": ["ØªØ¬Ø§ÙˆØ² 1", "ØªØ¬Ø§ÙˆØ² 2"],
  "summary_ar": "Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚ Ù‡Ù†Ø§"
}}"""

    async def analyze_cross_check(self, ahlya_data: dict, jort_data: dict, rne_data: dict) -> dict:
        """ExÃ©cute l'analyse croisÃ©e via l'API REST Gemini (v1 stable)."""

        company_name = ahlya_data.get("name", "Unknown")

        if not self.api_key:
            logger.error(f"LLM analysis skipped for '{company_name}': no API key")
            return _fallback_response("no_api_key", "GEMINI_API_KEY ØºÙŠØ± Ù…ÙØ¹ÙÙŠÙÙ‘Ù†")

        logger.info(f"ğŸ” Starting LLM cross-check for: {company_name}")
        start_time = datetime.now()
        prompt = self._build_prompt(ahlya_data, jort_data, rne_data)

        # â”€â”€ Build the REST API request body â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        request_body = {
            "system_instruction": {
                "parts": [{"text": SYSTEM_PROMPT}]
            },
            "contents": [
                {
                    "parts": [{"text": prompt}]
                }
            ],
            "generationConfig": {
                "temperature": 0.0,
                "topP": 1,
                "topK": 1,
                "responseMimeType": "application/json"
            }
        }

        url = f"{GEMINI_ENDPOINT}?key={self.api_key}"

        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    url,
                    json=request_body,
                    headers={"Content-Type": "application/json"}
                )

            # â”€â”€ Handle HTTP errors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if response.status_code == 429:
                logger.warning(f"âš ï¸ Rate-limit Gemini (429) for '{company_name}'")
                return _fallback_response("rate_limited", "Ø§Ù„Ø®Ø¯Ù…Ø© Ù…Ø´ØºÙˆÙ„Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§.")

            if response.status_code != 200:
                error_detail = response.text[:300]
                logger.error(f"âŒ Gemini API {response.status_code} for '{company_name}': {error_detail}")
                return _fallback_response(f"http_{response.status_code}", error_detail)

            # â”€â”€ Parse the response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            resp_json = response.json()
            candidates = resp_json.get("candidates", [])
            if not candidates:
                logger.error(f"âŒ No candidates in Gemini response for '{company_name}'")
                return _fallback_response("no_candidates", "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")

            text = candidates[0].get("content", {}).get("parts", [{}])[0].get("text", "")
            result = json.loads(text)

            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(
                f"âœ… Analysis complete for '{company_name}' â€” "
                f"score={result.get('match_score')}, status={result.get('status')}, "
                f"time={elapsed:.1f}s"
            )
            return result

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONDecodeError for '{company_name}': {e}")
            return _fallback_response("json_parse_error", "ØªØ¹Ø°Ù‘Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")

        except httpx.TimeoutException:
            logger.error(f"âŒ Timeout for '{company_name}' (60s limit)")
            return _fallback_response("timeout", "Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")

        except Exception as e:
            logger.error(f"âŒ Unexpected error for '{company_name}': {e}")
            return _fallback_response("unexpected_error", str(e))

# Instance unique du service
llm_service = LLMAnalysisService()